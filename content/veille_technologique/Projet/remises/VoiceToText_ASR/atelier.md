+++
title = "Atelier pratique : Voice-to-Text"
weight = 3
+++

---

## üìã Objectifs du laboratoire

√Ä la fin de ce laboratoire, vous serez capable de :
- Comprendre les bases de la reconnaissance vocale automatique (ASR)
- Installer et configurer un environnement Docker pour l'ASR
- Utiliser Whisper d'OpenAI pour transcrire de l'audio en texte
- Traiter diff√©rents formats audio et langues
- Cr√©er un script Python pour automatiser la transcription

**Dur√©e estim√©e**: 2h30 √† 3h00

---

##  Pr√©requis

- Docker install√© sur votre machine
- Fichiers audio de test (ou vous pouvez enregistrer votre voix)
- 4 GB d'espace disque disponible
- Connexion Internet pour t√©l√©charger les mod√®les
- D√©p√¥t git: https://github.com/Marwan-Maalouf/atelier-ASR.git

---

##  Partie 1: Installation et configuration

**Dur√©e**: 30-45 minutes

### √âtape 1.1: Pr√©parer l'environnement

Cr√©ez un dossier pour le laboratoire dans le repertoire:

```bash
mkdir lab-asr
cd lab-asr
mkdir audio_samples
mkdir transcriptions
```

### √âtape 1.2: Cr√©er le Dockerfile

Cr√©ez un fichier nomm√© `Dockerfile` avec le contenu suivant:

```dockerfile
# Image de base Ubuntu
FROM ubuntu:22.04

# √âviter les prompts interactifs
ENV DEBIAN_FRONTEND=noninteractive

# Mise √† jour et installation des d√©pendances
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    ffmpeg \
    git \
    nano \
    wget \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Installation de Whisper et ses d√©pendances
RUN pip3 install --no-cache-dir \
    openai-whisper \
    pydub \
    torch \
    numpy

# Cr√©er les r√©pertoires de travail
WORKDIR /workspace
RUN mkdir -p /workspace/audio_samples /workspace/transcriptions

# Point d'entr√©e
CMD ["/bin/bash"]
```

### √âtape 1.3: Construire l'image Docker

```bash
docker build -t lab-asr:latest .
```

{{< notice info >}}
 Cette √©tape peut prendre 5-10 minutes selon votre connexion Internet.
{{< /notice >}}

### √âtape 1.4: Lancer le conteneur

```bash
docker run -it --rm \
  -v $(pwd)/audio_samples:/workspace/audio_samples \
  -v $(pwd)/transcriptions:/workspace/transcriptions \
  --name asr-lab \
  lab-asr:latest
```

{{< notice success >}}
**Point de v√©rification**: Vous devriez maintenant √™tre dans le conteneur avec un prompt qui ressemble √† `root@xxxxx:/workspace#`
{{< /notice >}}

---

##  Partie 2: Pr√©parer des √©chantillons audio

**Dur√©e**: 15-20 minutes

### √âtape 2.1: T√©l√©charger des √©chantillons audio de test

Dans le conteneur, t√©l√©chargez quelques exemples:

```bash
# Exemple en anglais
wget -O /audio_samples/Enregistrement.m4a \
  "https://www2.cs.uic.edu/~i101/SoundFiles/BabyElephantWalk60.wav"

# Vous pouvez aussi copier vos propres fichiers audio
# depuis votre machine h√¥te vers le dossier audio_samples
```

### √âtape 2.2: Cr√©er vos propres √©chantillons audio

Sortez du conteneur (tapez `exit`) et cr√©ez des fichiers audio de test.

{{< notice tip >}}
 **Conseil**: Enregistrez 2-3 phrases claires en fran√ßais avec:
- **Audacity** (Linux)
- **GNOME Sound Recorder**
- Votre t√©l√©phone, puis transf√©rez le fichier

Sauvegardez en MP3 ou WAV dans le dossier `audio_samples/`
{{< /notice >}}

**Exemples de phrases √† enregistrer**:
- "Bonjour, ceci est un test de reconnaissance vocale automatique."
- "La technologie ASR permet de convertir la parole en texte."
- "Docker facilite le d√©ploiement d'applications complexes."

---

##  Partie 3: Premiers pas avec Whisper

**Dur√©e**: 30-40 minutes

Retournez dans le conteneur:

```bash
docker start -i asr-lab
# ou si vous l'aviez arr√™t√©:
docker run -it --rm \
  -v $(pwd)/audio_samples:/workspace/audio_samples \
  -v $(pwd)/transcriptions:/workspace/transcriptions \
  --name asr-lab \
  lab-asr:latest
```

### √âtape 3.1: Test basique avec Whisper CLI

```bash
# Transcrire un fichier audio
whisper /workspace/audio_samples/sample_en.mp3 --model tiny --language English
```

** Observations √† noter**:
- Temps de traitement
- Pr√©cision de la transcription
- Fichiers g√©n√©r√©s (txt, json, srt, etc.)

### √âtape 3.2: Comprendre les mod√®les Whisper

Whisper propose plusieurs tailles de mod√®les:

| Mod√®le | Param√®tres | Taille | Vitesse | Pr√©cision |
|--------|-----------|--------|---------|-----------|
| tiny   | 39 M      | ~75 MB | Tr√®s rapide | Basique |
| base   | 74 M      | ~142 MB | Rapide | Bonne |
| small  | 244 M     | ~466 MB | Moyen | Tr√®s bonne |
| medium | 769 M     | ~1.5 GB | Lent | Excellente |
| large  | 1550 M    | ~3 GB | Tr√®s lent | Maximale |

{{< notice info >}}
üí° Pour ce laboratoire, nous utiliserons principalement **tiny** et **base** pour la rapidit√©.
{{< /notice >}}

---

## üíªPartie 4: Scripts Python pour la transcription

**Dur√©e**: 45-60 minutes

### Exercice 1: Script de transcription simple

Cr√©ez un fichier `transcribe_simple.py`:

```python
#!/usr/bin/env python3
import whisper
import sys
import os

def transcribe_audio(audio_path, model_name="base"):
    """
    Transcrit un fichier audio en texte
    
    Args:
        audio_path: Chemin vers le fichier audio
        model_name: Nom du mod√®le Whisper √† utiliser
    
    Returns:
        Texte transcrit
    """
    print(f" Chargement du mod√®le '{model_name}'...")
    model = whisper.load_model(model_name)
    
    print(f"üé§ Transcription de: {audio_path}")
    result = model.transcribe(audio_path)
    
    return result

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python3 transcribe_simple.py <fichier_audio>")
        sys.exit(1)
    
    audio_file = sys.argv[1]
    
    if not os.path.exists(audio_file):
        print(f" Erreur: Le fichier '{audio_file}' n'existe pas")
        sys.exit(1)
    
    # Transcription
    result = transcribe_audio(audio_file)
    
    # Affichage des r√©sultats
    print("\n" + "="*50)
    print(" TRANSCRIPTION:")
    print("="*50)
    print(result["text"])
    print("="*50)
    print(f" Langue d√©tect√©e: {result['language']}")
```

**Testez le script**:

```bash
python3 transcribe_simple.py /workspace/audio_samples/sample_en.mp3
```

### Exercice 2: Script avanc√© avec sauvegarde et timestamps

Cr√©ez `transcribe_advanced.py`:

```python
#!/usr/bin/env python3
import whisper
import json
import os
from datetime import datetime

def transcribe_with_timestamps(audio_path, model_name="base", language=None):
    """
    Transcrit un fichier audio avec timestamps et sauvegarde les r√©sultats
    """
    # Chargement du mod√®le
    print(f"üîÑ Chargement du mod√®le '{model_name}'...")
    model = whisper.load_model(model_name)
    
    # Options de transcription
    options = {"fp16": False}
    if language:
        options["language"] = language
    
    # Transcription
    print(f"üé§ Transcription en cours...")
    result = model.transcribe(audio_path, **options)
    
    return result

def save_transcription(result, audio_path, output_dir="/workspace/transcriptions"):
    """
    Sauvegarde la transcription dans diff√©rents formats
    """
    # Cr√©er le nom de base pour les fichiers de sortie
    base_name = os.path.splitext(os.path.basename(audio_path))[0]
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 1. Sauvegarder le texte complet
    txt_path = os.path.join(output_dir, f"{base_name}_{timestamp}.txt")
    with open(txt_path, "w", encoding="utf-8") as f:
        f.write(result["text"])
    print(f" Texte sauvegard√©: {txt_path}")
    
    # 2. Sauvegarder avec timestamps (format JSON)
    json_path = os.path.join(output_dir, f"{base_name}_{timestamp}.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f" JSON sauvegard√©: {json_path}")
    
    # 3. Sauvegarder au format SRT (sous-titres)
    srt_path = os.path.join(output_dir, f"{base_name}_{timestamp}.srt")
    write_srt(result["segments"], srt_path)
    print(f" SRT sauvegard√©: {srt_path}")
    
    return txt_path, json_path, srt_path

def write_srt(segments, output_path):
    """
    √âcrit un fichier SRT (sous-titres) √† partir des segments
    """
    with open(output_path, "w", encoding="utf-8") as f:
        for i, segment in enumerate(segments, start=1):
            start_time = format_timestamp(segment["start"])
            end_time = format_timestamp(segment["end"])
            text = segment["text"].strip()
            
            f.write(f"{i}\n")
            f.write(f"{start_time} --> {end_time}\n")
            f.write(f"{text}\n\n")

def format_timestamp(seconds):
    """
    Formate les secondes en format SRT (HH:MM:SS,mmm)
    """
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

def display_segments(segments):
    """
    Affiche les segments avec timestamps
    """
    print("\n" + "="*70)
    print(" TRANSCRIPTION AVEC TIMESTAMPS:")
    print("="*70)
    
    for segment in segments:
        start = segment["start"]
        end = segment["end"]
        text = segment["text"].strip()
        print(f"[{start:.2f}s ‚Üí {end:.2f}s] {text}")
    
    print("="*70)

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python3 transcribe_advanced.py <fichier_audio> [mod√®le] [langue]")
        print("Exemple: python3 transcribe_advanced.py audio.mp3 base fr")
        sys.exit(1)
    
    audio_file = sys.argv[1]
    model_name = sys.argv[2] if len(sys.argv) > 2 else "base"
    language = sys.argv[3] if len(sys.argv) > 3 else None
    
    if not os.path.exists(audio_file):
        print(f" Erreur: Le fichier '{audio_file}' n'existe pas")
        sys.exit(1)
    
    # Transcription
    result = transcribe_with_timestamps(audio_file, model_name, language)
    
    # Affichage
    print(f"\n Langue d√©tect√©e: {result['language']}")
    display_segments(result["segments"])
    
    # Sauvegarde
    print("\n Sauvegarde des r√©sultats...")
    save_transcription(result, audio_file)
    
    print("\n Transcription termin√©e avec succ√®s!")
```

**Testez le script avanc√©**:

```bash
python3 transcribe_advanced.py /workspace/audio_samples/sample_en.mp3 base en
```

---

##  Partie 5: Exercices pratiques

**Dur√©e**: 30-45 minutes

### Exercice 3: Traitement par lot (Batch Processing)

**Objectif**: Cr√©er un script qui transcrit tous les fichiers audio d'un dossier

**Instructions**:
1. Cr√©ez un fichier `batch_transcribe.py`
2. Le script doit:
   - Parcourir tous les fichiers .mp3, .wav, .m4a dans un dossier
   - Transcrire chaque fichier
   - Sauvegarder les r√©sultats
   - Cr√©er un rapport r√©capitulatif (nombre de fichiers, dur√©e totale, etc.)

{{< notice tip >}}
** Indice**: Utilisez `os.listdir()` et une boucle `for` pour parcourir les fichiers
{{< /notice >}}

### Exercice 4: D√©tection de langue automatique

**Objectif**: Modifier le script pour d√©tecter automatiquement la langue

**Instructions**:
1. Cr√©ez un fichier `detect_language.py`
2. Transcrivez le fichier SANS sp√©cifier la langue
3. Affichez la langue d√©tect√©e et le niveau de confiance
4. Testez avec des fichiers dans diff√©rentes langues (fran√ßais, anglais, espagnol)

### Exercice 5: Filtrage et nettoyage

**Objectif**: Am√©liorer la qualit√© de la transcription

**Instructions**:
1. Cr√©ez un fichier `clean_transcription.py`
2. Ajoutez des fonctions pour:
   - Supprimer les doublons de mots
   - Corriger la capitalisation (premi√®re lettre des phrases)
   - Supprimer les mots de remplissage (euh, hmm, etc.)
   - Ajouter la ponctuation si manquante

{{< notice tip >}}
** Indice**: Utilisez des expressions r√©guli√®res (module `re`)
{{< /notice >}}

---

## üìä Partie 6: Analyse et comparaison

**Dur√©e**: 20-30 minutes

### Exercice 6: Benchmark des mod√®les

Cr√©ez `benchmark_models.py`:

```python
#!/usr/bin/env python3
import whisper
import time

def benchmark_model(audio_path, model_name):
    """
    Teste la performance d'un mod√®le Whisper
    """
    print(f"\n Test du mod√®le: {model_name}")
    
    # Chargement
    start_load = time.time()
    model = whisper.load_model(model_name)
    load_time = time.time() - start_load
    
    # Transcription
    start_transcribe = time.time()
    result = model.transcribe(audio_path)
    transcribe_time = time.time() - start_transcribe
    
    return {
        "model": model_name,
        "load_time": load_time,
        "transcribe_time": transcribe_time,
        "total_time": load_time + transcribe_time,
        "text": result["text"],
        "language": result["language"]
    }

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python3 benchmark_models.py <fichier_audio>")
        sys.exit(1)
    
    audio_file = sys.argv[1]
    models = ["tiny", "base"]
    
    results = []
    for model_name in models:
        result = benchmark_model(audio_file, model_name)
        results.append(result)
    
    # Affichage comparatif
    print("\n" + "="*70)
    print("R√âSULTATS DU BENCHMARK")
    print("="*70)
    
    for r in results:
        print(f"\nMod√®le: {r['model']}")
        print(f"  Temps de chargement: {r['load_time']:.2f}s")
        print(f"  Temps de transcription: {r['transcribe_time']:.2f}s")
        print(f"  Temps total: {r['total_time']:.2f}s")
        print(f"  Langue: {r['language']}")
        print(f"  Aper√ßu: {r['text'][:100]}...")
```

{{< notice question >}}
**Question de r√©flexion**: Quel mod√®le choisiriez-vous pour une application en temps r√©el? Pourquoi?
{{< /notice >}}

---

## üéì Questions de compr√©hension

1. **Quelle est la diff√©rence entre les mod√®les tiny, base et large?**

2. **Pourquoi est-il important de sp√©cifier la langue lors de la transcription?**

3. **Dans quel format sont sauvegard√©s les timestamps? Pourquoi le format SRT est-il utile?**

4. **Quels sont les avantages et inconv√©nients d'utiliser Docker pour ce type de projet?**

5. **Comment pourriez-vous am√©liorer la pr√©cision de la transcription pour:**
   - Un accent r√©gional fort?
   - De la musique en arri√®re-plan?
   - Des termes techniques sp√©cialis√©s?

---

√Ä la fin du laboratoire, cr√©ez un fichier `RAPPORT.md` contenant:

1. **R√©sum√© des apprentissages** (5-10 lignes)
2. **Difficult√©s rencontr√©es** et solutions
3. **R√©sultats des benchmarks** (tableau comparatif)
4. **Captures d'√©cran** des transcriptions r√©ussies
5. **Id√©es d'am√©lioration** ou d'applications futures

---

## Pour aller plus loin

- Int√©grer Whisper dans une application web (Flask/Django)
- Traiter des flux audio en temps r√©el
- Fine-tuner un mod√®le pour un vocabulaire sp√©cifique
- Combiner ASR avec de la synth√®se vocale (TTS) pour un assistant vocal
- Ajouter de la d√©tection d'√©motion dans la voix

---

## Ressources suppl√©mentaires

- [Documentation officielle Whisper](https://github.com/openai/whisper)
- [Guide des formats audio](https://en.wikipedia.org/wiki/Audio_file_format)
- [Paper original de Whisper](https://arxiv.org/abs/2212.04356)
- [Forum de support](https://github.com/openai/whisper/discussions)

---

## Checklist de compl√©tion

- [ ] Installation Docker r√©ussie
- [ ] Transcription d'au moins 3 fichiers audio diff√©rents
- [ ] Tests avec 2 mod√®les diff√©rents (tiny et base)
- [ ] Script de transcription simple fonctionnel
- [ ] Script avanc√© avec timestamps fonctionnel
- [ ] Au moins 2 exercices pratiques compl√©t√©s
- [ ] Benchmark r√©alis√©
- [ ] Rapport de laboratoire r√©dig√©

---


*N'h√©sitez pas √† exp√©rimenter et √† poser des questions si vous rencontrez des difficult√©s.*


## Corrig√©

- [Corrig√©](./corrige/) 